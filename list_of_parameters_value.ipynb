{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1 Hyperparameters \n",
    "# #  1.1 Optimizer\n",
    "# #  While Stochastic Gradient Descent is used in many Neural Network problems, it has the problem of converging to a local minimum. \n",
    "# This of course presents a problem considering Bitcoin price. Some other nice optimizers are variations of adaptive learning algorithms, like Adam, Adagrad, \n",
    "# and RMSProp. Adam was found to work slightly better than the rest, and thatâ€™s why we go for it. \n",
    "\n",
    "# # 1.2 Loss function \n",
    "# # The performance measure for regression problems, will typically be either RMSE (Root Mean Square Error) \n",
    "# or MAE (Mean Absolute Error).RMSE is generally used when distribution resembles a bell-shaped curve, but \n",
    "# given the Bitcoin price spikes we chose to go MAE, since it deals better with outliers.\n",
    "\n",
    "# # 1.3 Activation function\n",
    "# # The choice for activation function was not very difficult. The most popular are sigmoid, tanh, and ReLu.\n",
    "# Sigmoid suffers from vanishing gradient, therefore almost no signal flows from the neuron to its weight, \n",
    "# moreover it is not centered around zero, as a result the gradient might be to high or to low a number. \n",
    "# By contrast, tanh makes the output zero centered, and in practice is almost always preferred to sigmoid. \n",
    "# ReLu is also widely used, and since it was invented later, it should be better. Nevertheless, for \n",
    "# predicting Bitcoin price that was not the case, and we chose tanh due to better results.\n",
    "\n",
    "# # 1.4 Number of Neurons in hidden layers \n",
    "# # We opted for 64 neurons in the hidden layers, it actually costs a lot to have more neurons, \n",
    "# as the training process will last longer. Also, trying a larger number did not give improved results.\n",
    "\n",
    "## 1.5 Epochs Rather arbitrarily, we decided for 30 epochs, after trying other values, like 50, or 20.\n",
    "# As with the number of hidden layer neurons, the more epochs, the more time it takes for training to \n",
    "# finish, since one epoch is a full iteration over the training data. Also, it may overfit the model. \n",
    "\n",
    "# # 1.6 Batch size\n",
    "# # With respect to the batch size.We tried several values and decide to use 500.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size \t50\t200\t500-1000\n",
    "# number of neurons\t64\t128\t256\n",
    "# window size \t3\t7\t10\n",
    "# loss function \tmae\tmse\t\n",
    "# optimizer\tadam \tsgd\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
